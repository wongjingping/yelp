{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple-Instance Learning with Yelp Restaurant Dataset\n",
    "\n",
    "This notebook aims to train a classifier for the business IDs using the aggregated features extracted from the final dense layer of a VGG16 network pre-trained on ImageNet data. You can download the weights [here](https://drive.google.com/file/d/0Bz7KyqmuGsilT0J5dmRCM0ROVHc/view?usp=sharing), courtesy of [baraldilorenzo](https://gist.github.com/baraldilorenzo/07d7802847aaad0a35d3).\n",
    "\n",
    "\n",
    "## 1) Extraction of Image Features using pre-trained VGG16 model in Keras\n",
    "\n",
    "To begin, we need to download the model weights and load them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: Tesla K40c (CNMeM is disabled, CuDNN not available)\n",
      "/opt/anaconda2/lib/python2.7/site-packages/theano/tensor/signal/downsample.py:5: UserWarning: downsample module has been moved to the pool module.\n",
      "  warnings.warn(\"downsample module has been moved to the pool module.\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "from time import time\n",
    "from PIL import Image\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Flatten, Dense, Dropout\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "\n",
    "\n",
    "# build VGG-16 model\n",
    "def VGG_16(weights_path=None):\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(ZeroPadding2D((1,1),input_shape=(3,224,224)))\n",
    "    model.add(Convolution2D(64, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(64, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(128, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(128, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1000, activation='softmax'))\n",
    "\n",
    "    if weights_path:\n",
    "        model.load_weights(weights_path)\n",
    "\n",
    "    # remove final softmax layer\n",
    "    fc7 = Sequential()\n",
    "    depth = len(model.layers)\n",
    "    for i in range(depth-1):\n",
    "       fc7.add(model.layers[i])\n",
    "    # compile to get predict function. arbitrary optimizer and loss ok\n",
    "    fc7.compile(optimizer='sgd',loss='mse') \n",
    "\n",
    "    return fc7\n",
    "\n",
    "# build model\n",
    "model = VGG_16('models/vgg16_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we read in the image data, and store the features from the final dense layer (4096-dim). \n",
    "Takes over 12 hours using a GPU for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_,ch,h,w = model.input_shape\n",
    "_,nf = model.output_shape\n",
    "\n",
    "# store training features\n",
    "def store_feat(is_train, batch_size=128):\n",
    "    t_read = time()\n",
    "    if is_train:\n",
    "        h5_path, photo_path = 'data/train_feat.h5','data/train_photos'\n",
    "        photo_biz = pd.read_csv('data/train_photo_to_biz_ids.csv')\n",
    "    else:\n",
    "        h5_path, photo_path = 'data/test_feat.h5','data/test_photos'\n",
    "        photo_biz = pd.read_csv('data/test_photo_to_biz.csv')\n",
    "    mean_pixels = np.array([123.68,116.779,103.939]).reshape((3,1,1))\n",
    "    feat = h5py.File(h5_path,'w')\n",
    "    X = feat.create_dataset(\n",
    "        name='X',\n",
    "        shape=(len(photo_biz),nf),\n",
    "        dtype=np.float32)\n",
    "    for b in range(len(photo_biz)/batch_size+1):\n",
    "        i_lo, i_hi = b*batch_size, min((b+1)*batch_size,len(photo_biz))\n",
    "        batch_size_i = i_hi - i_lo\n",
    "        raw_i = np.zeros((batch_size_i,ch,h,w))\n",
    "        for i in range(i_lo,i_hi):\n",
    "            f_ = '%s/%s.jpg' % (photo_path,photo_biz['photo_id'][i])\n",
    "            im_j = Image.open(f_).resize((w,h))\n",
    "            ima_j = np.asarray(im_j).transpose((2,0,1))\n",
    "            raw_i[i-i_lo,:,:,:] = ima_j - mean_pixels\n",
    "        X[i_lo:i_hi,:] = model.predict(raw_i,batch_size_i)\n",
    "        if i_hi % 1024 == 0:\n",
    "            print('reading %ith photo took %ds' % (i_hi,time()-t_read))\n",
    "    feat.close()\n",
    "\n",
    "store_feat(is_train=True)\n",
    "store_feat(is_train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Aggregate Features by Business ID\n",
    "\n",
    "We perform a simple aggregation of our data over the business ID's by taking the mean, std, min, max of each of the 4096 features, and add a count of the number of pictures associated with the business. That should give us a (2000 by 4096x4+1) matrix. \n",
    "\n",
    "Let's begin by reading in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read in csv data\n",
    "train_biz_lab = pd.read_csv('data/train.csv')\n",
    "test_biz_lab = pd.read_csv('data/sample_submission.csv')\n",
    "train_photo_biz = pd.read_csv('data/train_photo_to_biz_ids.csv')\n",
    "test_photo_biz = pd.read_csv('data/test_photo_to_biz.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function for calculating the various stats of the 4096 features for each business ID. That should give us a (2000 by 4096x4+1) and a (10000 by 4096x4+1) matrix for the train and test sets respectively.\n",
    "\n",
    "Let's perform the aggregation over the business_ids, and save the aggregated feature information in another hdf5 file. Takes about 7 minutes on my red hat server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function for aggregating feature data\n",
    "def get_agg(is_train):\n",
    "    t_start = time()\n",
    "    if is_train:\n",
    "        print('Aggregating for training data')\n",
    "        feat_path, agg_path = 'data/train_feat.h5', 'data/train_agg_feat.h5'\n",
    "        photo_biz, biz_lab = train_photo_biz, train_biz_lab\n",
    "    else:\n",
    "        print('Aggregating for testing data')\n",
    "        feat_path, agg_path = 'data/test_feat.h5', 'data/test_agg_feat.h5'\n",
    "        photo_biz, biz_lab = test_photo_biz, test_biz_lab\n",
    "    nrow, ncol = len(biz_lab), 4*4096+1\n",
    "    feat = h5py.File(feat_path,'r')\n",
    "    agg = h5py.File(agg_path,'w')\n",
    "    Xa = agg.create_dataset(\n",
    "        name='X',\n",
    "        shape=(nrow,ncol),\n",
    "        dtype=np.float32)\n",
    "    for i in biz_lab.index:\n",
    "        biz_i = biz_lab['business_id'][i]\n",
    "        photo_ix = photo_biz.index[photo_biz['business_id']==biz_i]\n",
    "        biz_mean = feat['X'][photo_ix].mean(axis=0)\n",
    "        biz_std = feat['X'][photo_ix].std(axis=0)\n",
    "        biz_max = feat['X'][photo_ix].max(axis=0)\n",
    "        biz_min = feat['X'][photo_ix].min(axis=0)\n",
    "        Xa[i,:] = np.hstack([biz_mean,biz_std,biz_max,biz_min,len(photo_ix)])\n",
    "        if i % 200 == 0:\n",
    "            print('Done with %dth business' % i)\n",
    "    feat.close()\n",
    "    agg.close()\n",
    "    print('Took %ds' % (time()-t_start))\n",
    "\n",
    "get_agg(is_train=True)\n",
    "get_agg(is_train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Train a SVM on the aggregated features\n",
    "\n",
    "Having extracted and aggregated the features, we now train a multi-label classifier with some nice functions from sklearn. For easy re-use, we wrap our cross-validation with svm in a function to facilitate easy training with different parameter sets, especially if you're planning to parallelize the parameter tuning.\n",
    "\n",
    "Some things to take note:\n",
    " - We have a case where n_examples (2k) << n_features (16k). As such we might not need to 'project' into high dimensional feature space using a rbf kernel. Moreover our features also captures some second-order statistics. A useful guide [here](www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf) has more tips on training a SVM classifier.\n",
    " - Smaller C corresponds to more regularization\n",
    " - You can choose to scale or not, but the optimal parameters may differ depending on that choice. linear kernels aren't that sensitive to scaling, while rbf kernels typically require it to work well with the default parameters (gamma).\n",
    " - Each model takes about 6mins+ to train on my red hat server using 1 thread. A 5-fold cv for a set of parameters would take about over half an hour to run. Feel free to parallelize it by spawning multiple processes or distributing it to a cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import sklearn functions\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import svm\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# read in training data from hdf5\n",
    "feat = h5py.File('data/train_agg_feat.h5','r')\n",
    "X = feat['X'][...]\n",
    "feat.close()\n",
    "\n",
    "# read labels for training data\n",
    "def to_bool(x):\n",
    "    return(pd.Series([1L if str(i) in str(x).split(' ') else 0L for i in range(9)]))\n",
    "y = np.asarray(train_biz_lab['labels'].apply(to_bool))\n",
    "\n",
    "# function for cross-validating svm\n",
    "def svm_cv(X,y,kernel='linear',C=.1,scale=False,nfolds=5,save_path=None):\n",
    "    scaler = StandardScaler()\n",
    "    clf = OneVsRestClassifier(svm.SVC(kernel=kernel,C=C, random_state=290615))\n",
    "    cv = KFold(n=len(X),n_folds=nfolds,shuffle=True,random_state=290615)\n",
    "    F1cv = np.zeros(nfolds)\n",
    "    for i, (a,b) in enumerate(cv):\n",
    "        t_i = time()\n",
    "        if scale:\n",
    "            X_train = scaler.fit_transform(X[a])\n",
    "            X_test = scaler.transform(X[b])\n",
    "        else:\n",
    "            X_train = X[a]\n",
    "            X_test = X[b]\n",
    "        y_train = y[a]\n",
    "        y_test = y[b]\n",
    "        clf.fit(X_train,y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        F1cv[i] = f1_score(y_test, y_pred, average='micro')\n",
    "        print('F1 per class:' + str(f1_score(y_test, y_pred, average=None)))\n",
    "        print('Fold %d F1 %.3f took %ds' % (i, F1cv[i], time()-t_i))\n",
    "    print('Average F1 was %.3f' % (F1cv.mean()))\n",
    "    # append cv results to csv file and save it\n",
    "    if save_path:\n",
    "        cv_res = pd.DataFrame({'kernel':kernel,'C':C,'nfolds':nfolds,'scale':scale,'F1cv':[F1cv],'F1':F1cv.mean()})\n",
    "        cv_history = pd.read_csv('data/cv_history.csv')\n",
    "        cv_history = cv_history.append(cv_res)\n",
    "        cv_history.to_csv('data/cv_history.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now train various combinations by calling the function in an interactive fashion. Here's the best one that worked for me :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svm_cv(X,y,kernel='rbf',C=3,scale=True,save_path='data/cv_history.csv') # avg F1 = 0.769"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've cross-validated our model parameters, let's predict on the test set (submission data)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fit on full training data\n",
    "scaler = StandardScaler()\n",
    "X_sc = scaler.fit_transform(X)\n",
    "clf = OneVsRestClassifier(svm.SVC(kernel='rbf',C=3., random_state=290615))\n",
    "clf.fit(X_sc,y)\n",
    "\n",
    "# read in aggregated features from hdf5 # ~ 39 mins\n",
    "feat = h5py.File('data/test_agg_feat.h5','r')\n",
    "X_test = scaler.transform(feat['X'][...])\n",
    "feat.close()\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# convert boolean matrix to integer labels\n",
    "y_int = np.where(y_pred)\n",
    "df = pd.DataFrame({'biz':y_int[0],'int':y_int[1]})\n",
    "test_biz_lab['labels'] = df.groupby('biz').apply(lambda x: ' '.join([str(i) for i in x['int']]))\n",
    "test_biz_lab.to_csv('data/sub5_vgg_svm.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
